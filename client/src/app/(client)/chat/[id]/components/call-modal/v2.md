"use client";

import { useCallback, useEffect, useRef, useState } from "react";
import { useMutation } from "@tanstack/react-query";
import { respondToCharacterCallAction } from "@/server/actions/call.actions";

type Status = "calling" | "idle" | "listening" | "responding";

interface Message {
  role: "USER" | "ASSISTANT";
  content: string;
}

interface Options {
  onError?: (e: Error) => void;
  silenceTimeoutMs?: number;
  voice: "male" | "female";
  characterId: string;
}

interface RespondResult {
  success: boolean;
  response?: string;
  messages?: Message[];
  error?: { message: string };
}

// Dummy placeholder for server action import - replace with your actual import path
// import { respondToCharacterCallAction } from "../server-actions/respondToCharacterCallAction";

export function useVoiceChat({
  onError,
  silenceTimeoutMs = 1000,
  voice = "male",
  characterId,
}: Options) {
  /* ---------- State ---------- */
  const [status, setStatus] = useState<Status>("idle");
  const [transcript, setTranscript] = useState("");
  const [aiResponse, setAiResponse] = useState<string | null>(null);
  const [soundLevel, setSoundLevel] = useState(0);
  const [isMuted, setIsMuted] = useState(false);
  const [messages, setMessages] = useState<Message[]>([]); // Persisted chat messages

  /* ---------- Refs ---------- */
  const recogRef = useRef<any | null>(null);
  const silenceTimer = useRef<NodeJS.Timeout | null>(null);
  const audioCtxRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const meterTick = useRef<NodeJS.Timeout | null>(null);
  const micStreamRef = useRef<MediaStream | null>(null);
  const started = useRef(false);

  /* ---------- React Query mutation for server action ---------- */
  const mutation = useMutation({
    mutationFn: ({ message }: { message: string }) =>
      respondToCharacterCallAction({ messages, message, characterId }),
  });

  /* ---------- Audio cleanup ---------- */
  const cleanupAudio = () => {
    if (meterTick.current) {
      clearInterval(meterTick.current);
      meterTick.current = null;
    }
    if (analyserRef.current) {
      analyserRef.current.disconnect();
      analyserRef.current = null;
    }
    if (audioCtxRef.current) {
      audioCtxRef.current.close();
      audioCtxRef.current = null;
    }
    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((t) => t.stop());
      micStreamRef.current = null;
    }
    setSoundLevel(0);
  };

  /* ---------- Sound meter for UI ---------- */
  const startSoundMeter = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    micStreamRef.current = stream;

    const ctx = new AudioContext();
    audioCtxRef.current = ctx;

    const src = ctx.createMediaStreamSource(stream);
    const analyser = ctx.createAnalyser();
    analyser.fftSize = 256;
    src.connect(analyser);
    analyserRef.current = analyser;

    const data = new Uint8Array(analyser.frequencyBinCount);
    meterTick.current = setInterval(() => {
      analyser.getByteFrequencyData(data);
      setSoundLevel(Math.max(...data) / 255);
    }, 80);
  };

  const speak = (
    text: string,
    emotion: "neutral" | "happy" | "sad" = "neutral"
  ) => {
    const synth = window.speechSynthesis;
    if (!synth) {
      onError?.(new Error("Speech Synthesis unsupported"));
      return;
    }

    const utter = new SpeechSynthesisUtterance(text);

    const loadAndSpeak = () => {
      const voices = synth.getVoices();
      let selected: SpeechSynthesisVoice | undefined;

      if (voice === "male") {
        selected = voices.find((v) =>
          /daniel|english male|google uk english male/i.test(v.name)
        );
      } else if (voice === "female") {
        selected = voices.find((v) =>
          /zira|samantha|english female|google us english/i.test(v.name)
        );
      }

      if (selected) {
        utter.voice = selected;
      }
      switch (emotion) {
        case "happy":
          utter.pitch = 1.5; // higher pitch
          utter.rate = 1.2; // slightly faster
          break;
        case "sad":
          utter.pitch = 0.8; // lower pitch
          utter.rate = 0.9; // slower
          break;
        default:
          utter.pitch = 1.0;
          utter.rate = 1.0;
      }

      utter.onend = () => {
        setStatus("listening");
        setAiResponse(null);
        recognise();
      };

      synth.speak(utter);
    };

    // Wait for voices if not ready
    if (synth.getVoices().length === 0) {
      synth.onvoiceschanged = () => loadAndSpeak();
    } else {
      loadAndSpeak();
    }
  };

  /* ---------- Speech recognition & AI interaction ---------- */
  const recognise = useCallback(() => {
    const SR =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;

    if (!SR) {
      onError?.(new Error("Speech Recognition unsupported"));
      return;
    }

    const recog = new SR();
    recogRef.current = recog;
    recog.lang = "en-US";
    recog.interimResults = true;
    recog.continuous = true;

    let fullText = "";
    console.log("-------------", fullText);

    recog.onresult = (e: any) => {
      for (let i = e.resultIndex; i < e.results.length; i++) {
        const result = e.results[i];
        const phrase = result[0].transcript;

        if (result.isFinal) {
          fullText += `${phrase} `;
          setTranscript("");
        } else {
          setTranscript(phrase);
        }
      }

      if (silenceTimer.current) clearTimeout(silenceTimer.current);
      silenceTimer.current = setTimeout(() => recog.stop(), silenceTimeoutMs);
    };

    recog.onend = async () => {
      setTranscript("");
      fullText = fullText.trim();

      if (!fullText || isMuted) {
        recognise();
        return;
      }

      setStatus("responding");

      // Add user message to messages state
      setMessages((prev) => [...prev, { role: "USER", content: fullText }]);

      try {
        // Send current messages + new user message to the server action
        const result = await mutation.mutateAsync({
          message: fullText,
        });

        if (!result.success || !result.response) {
          onError?.(new Error(result.error?.message || "AI response failed"));
          setStatus("listening");
          return;
        }

        // Add AI response to messages state
        setMessages((prev) => [
          ...prev,
          { role: "ASSISTANT", content: result.response! },
        ]);

        setAiResponse(result.response);
        speak(result.response);
      } catch (e: any) {
        onError?.(e);
        setStatus("listening");
      }
    };

    recog.onerror = () => {
      onError?.(new Error("Speech Recognition error"));
    };

    recog.start();
    setStatus("listening");
  }, [
    characterId,
    isMuted,
    messages,
    mutation,
    onError,
    silenceTimeoutMs,
    speak,
  ]);

  /* ---------- Start voice chat ---------- */
  const start = async () => {
    if (started.current) return;
    started.current = true;

    try {
      await startSoundMeter();

      setStatus("calling"); // Simulate 2 seconds calling before listening

      setTimeout(() => {
        recognise();
      }, 2000);
    } catch {
      onError?.(new Error("Mic permission denied"));
    }
  };

  /* ---------- End / cleanup ---------- */
  const end = () => {
    if (silenceTimer.current) {
      clearTimeout(silenceTimer.current);
      silenceTimer.current = null;
    }

    if (recogRef.current) {
      try {
        recogRef.current.onresult = null;
        recogRef.current.onend = null;
        recogRef.current.onerror = null;
        recogRef.current.abort();
      } catch {}
      recogRef.current = null;
    }

    if (window.speechSynthesis.speaking) {
      window.speechSynthesis.cancel();
    }

    cleanupAudio();

    // Clear chat history on call end
    setMessages([]);
    setStatus("idle");
    setTranscript("");
    setAiResponse(null);
    started.current = false;
  };

  const toggleMute = () => setIsMuted((m) => !m);

  useEffect(() => {
    return () => {
      end();
    };
  }, []);

  return {
    status,
    transcript,
    aiResponse,
    soundLevel,
    isMuted,
    toggleMute,
    start,
    end,
    messages,
  };
}
